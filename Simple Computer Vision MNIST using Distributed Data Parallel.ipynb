{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f812d",
   "metadata": {},
   "source": [
    "### Distributed Data Parallel on a Simple Computer Vision Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82d3d1",
   "metadata": {},
   "source": [
    "This tutorial shows and example of using Tune to perform a simple training task.  The task consists of training a model on the MNIST dataset using Data Distributed Parallel and Pytorch.  Ray provides a nice wrapper function for DDP so that any trainable classifier can be used for deep learning.  The MNIST database The MNIST (Modified National Institute of Standards and Technology database is a large set of handwritten numbers.  The data is commonly used for training various image processing system (Wikipedia). Here we train a simple classifer for images, train_mnist, and leverage the paradign of distributed data parallel (DDP).  DDP distributes analyses in parallel across multiple nodes.  It is considered one of the fastest and most efficient algorithms.  It was originally designed by scientists at Meta (then called Facebook).  See references for more information.\n",
    "\n",
    "#### References:\n",
    "\n",
    "Pytorch Distributed Overview: [Pytorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
    "\n",
    "Distributed Data Parallel: [Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "\n",
    "Ray RLLib Documentation: [Ray RLLib Documentation](https://docs.ray.io/en/master/rllib.html)\n",
    "\n",
    "Ray Tune Documentation: [Ray Tune Documentation](https://docs.ray.io/en/master/tune/index.html)\n",
    "\n",
    "*This tutorial is adapted from the documentation for Ray version 1.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fed9c",
   "metadata": {},
   "source": [
    "#### Check the version of Ray, instantiate and instance of ray and check the number of nodes\n",
    "\n",
    "It is helpful to check your verison of Ray to make sure you have correct code.  Ray tends to get updated frequently and as of the time of this tutorial the latest stable version is 1.9.  You can adapt this tutorial to work with other versions of Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e7a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray, version 1.9.0\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40162af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "   service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "   service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "   _temp_dir='/domino/datasets/local/{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "   ray.util.connect(f\"{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3e7fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NodeID': '22b8f92ba357331f114bc8096d97e23e9fda4d0e9340a3025b6be54f',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.46.158',\n",
       "  'NodeManagerHostname': 'ray-61bce103fa3de41ad2934902-ray-worker-3',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/raylet',\n",
       "  'MetricsExportPort': 63824,\n",
       "  'alive': True,\n",
       "  'Resources': {'accelerator_type:V100': 1.0,\n",
       "   'memory': 41296992666.0,\n",
       "   'GPU': 1.0,\n",
       "   'CPU': 7.0,\n",
       "   'node:10.0.46.158': 1.0,\n",
       "   'object_store_memory': 17698711142.0}},\n",
       " {'NodeID': '34613c8b93a2c46e37a14440d9ed5fcde848e096707257d55868b0f1',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.40.69',\n",
       "  'NodeManagerHostname': 'ray-61bce103fa3de41ad2934902-ray-head-0',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/raylet',\n",
       "  'MetricsExportPort': 64517,\n",
       "  'alive': True,\n",
       "  'Resources': {'GPU': 1.0,\n",
       "   'memory': 35398275072.0,\n",
       "   'node:10.0.40.69': 1.0,\n",
       "   'CPU': 7.0,\n",
       "   'accelerator_type:V100': 1.0,\n",
       "   'object_store_memory': 17699137536.0}},\n",
       " {'NodeID': '165c937628b1fc915ccaa4ae67de47afc6056e1f2a27a2eeef3bd8c9',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.47.186',\n",
       "  'NodeManagerHostname': 'ray-61bce103fa3de41ad2934902-ray-worker-1',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/raylet',\n",
       "  'MetricsExportPort': 63275,\n",
       "  'alive': True,\n",
       "  'Resources': {'object_store_memory': 17698625126.0,\n",
       "   'accelerator_type:V100': 1.0,\n",
       "   'memory': 41296791962.0,\n",
       "   'node:10.0.47.186': 1.0,\n",
       "   'GPU': 1.0,\n",
       "   'CPU': 7.0}},\n",
       " {'NodeID': 'de743f36645a3583845b2deb23c3837d04099c53a7d258256147ba46',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.49.153',\n",
       "  'NodeManagerHostname': 'ray-61bce103fa3de41ad2934902-ray-worker-0',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/raylet',\n",
       "  'MetricsExportPort': 51026,\n",
       "  'alive': True,\n",
       "  'Resources': {'CPU': 7.0,\n",
       "   'GPU': 1.0,\n",
       "   'memory': 41296984064.0,\n",
       "   'node:10.0.49.153': 1.0,\n",
       "   'object_store_memory': 17698707456.0,\n",
       "   'accelerator_type:V100': 1.0}},\n",
       " {'NodeID': 'ce007839d51ed73da68eb1d49014911ea91ddf1a2a3b19a1b6ba7868',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.38.75',\n",
       "  'NodeManagerHostname': 'ray-61bce103fa3de41ad2934902-ray-worker-2',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2021-12-17_19-12-18_498860_1/sockets/raylet',\n",
       "  'MetricsExportPort': 39992,\n",
       "  'alive': True,\n",
       "  'Resources': {'CPU': 7.0,\n",
       "   'node:10.0.38.75': 1.0,\n",
       "   'memory': 41297009869.0,\n",
       "   'object_store_memory': 17698718515.0,\n",
       "   'accelerator_type:V100': 1.0,\n",
       "   'GPU': 1.0}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003b5a2",
   "metadata": {},
   "source": [
    "#### Setting Up Tune, Distributed Data Parallel Wrapper and Trainable classes\n",
    "\n",
    "In order to use Tune a trainable classier is required.  That is a classifer that is created and will be fed into the Tune training definition (tune.run).  Our classifier here is called 'train_mnist' and the trainable classifier is the first classifer under the function run_ddp.  We have a second snippet of code that uses tune.run to run the classifer.  For this example we are running the iterations only once, but the number of iteration can be modified in your project depending on how the code is set-up.  Try running the code and seeing the classifer being trained. The trained classifer will be saved to your local Domino directory at the location specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f79d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.examples.mnist_pytorch import (train, test, get_data_loaders,\n",
    "                                             ConvNet)\n",
    "from ray.tune.integration.torch import (DistributedTrainableCreator,\n",
    "                                        distributed_checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebd631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train_mnist(config, checkpoint_dir=False):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    model = ConvNet().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        with open(os.path.join(checkpoint_dir, \"checkpoint\")) as f:\n",
    "            model_state, optimizer_state = torch.load(f)\n",
    "\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    model = DistributedDataParallel(model)\n",
    "\n",
    "    for epoch in range(40):\n",
    "        train(model, optimizer, train_loader, device)\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            with distributed_checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "\n",
    "def run_ddp_tune(num_workers, num_gpus_per_worker, workers_per_node=None):\n",
    "    trainable_cls = DistributedTrainableCreator(\n",
    "        train_mnist,\n",
    "        num_workers=num_workers,\n",
    "        num_gpus_per_worker=num_gpus_per_worker,\n",
    "        num_workers_per_host=workers_per_node) #add in\n",
    "\n",
    "    analysis = tune.run(\n",
    "        trainable_cls,\n",
    "        num_samples=1,\n",
    "        stop={\"training_iteration\": 1},\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        sync_config=tune.SyncConfig(\n",
    "        syncer=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5efea7",
   "metadata": {},
   "source": [
    "#### Training the Classifer\n",
    "\n",
    "Here we train the classifer without using a sync configuration (that is we do not sync our log system to the cloud storage).  If you wish to sync to cloud storage instructions on how to do so are here: [Sync to Cloud Storage](!https://docs.ray.io/en/latest/tune/user-guide.html#tune-checkpoint-syncing).  During this process you will see three statuses for the run.  Those are pending, running and terminated.  Terminated indicates the run was successfully completed.  After training, you can choose the best checkpoint, call it back, and voìla, use it for predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523d1552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current time: 2021-12-17 19:14:15 (running for 00:00:04.31)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Memory usage on this node: 3.3/60.0 GiB\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Resources requested: 2.0/35 CPUs, 2.0/5 GPUs, 0.0/186.81 GiB heap, 0.0/82.42 GiB objects (0.0/5.0 accelerator_type:V100)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result logdir: /home/ubuntu/ray_results/WrappedDistributedTorchTrainable_2021-12-17_19-14-11\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | Trial name                                   | status   | loc            |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m |----------------------------------------------+----------+----------------|\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | WrappedDistributedTorchTrainable_83e1d_00000 | RUNNING  | 10.0.38.75:112 |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/ubuntu/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]158)\u001b[0m \n",
      "  4%|▍         | 384000/9912422 [00:00<00:02, 3839124.96it/s]\n",
      " 40%|████      | 3974144/9912422 [00:00<00:00, 22697234.80it/s]\n",
      "9913344it [00:00, 32939824.71it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Extracting /home/ubuntu/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/ubuntu/data/MNIST/raw\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/ubuntu/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current time: 2021-12-17 19:14:16 (running for 00:00:05.31)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Memory usage on this node: 3.3/60.0 GiB\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Resources requested: 2.0/35 CPUs, 2.0/5 GPUs, 0.0/186.81 GiB heap, 0.0/82.42 GiB objects (0.0/5.0 accelerator_type:V100)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result logdir: /home/ubuntu/ray_results/WrappedDistributedTorchTrainable_2021-12-17_19-14-11\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | Trial name                                   | status   | loc            |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m |----------------------------------------------+----------+----------------|\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | WrappedDistributedTorchTrainable_83e1d_00000 | RUNNING  | 10.0.38.75:112 |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \r",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\r",
      "29696it [00:00, 1044758.77it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Extracting /home/ubuntu/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/ubuntu/data/MNIST/raw\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/ubuntu/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \r",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \r",
      " 23%|██▎       | 371712/1648877 [00:00<00:00, 3684323.64it/s]\r",
      "1649664it [00:00, 10175297.78it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Extracting /home/ubuntu/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/ubuntu/data/MNIST/raw\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/ubuntu/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \r",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\r",
      "5120it [00:00, 23572817.21it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m Extracting /home/ubuntu/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ubuntu/data/MNIST/raw\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=105, ip=10.0.46.158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result for WrappedDistributedTorchTrainable_83e1d_00000:\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   date: 2021-12-17_19-14-21\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   experiment_id: 843829f8718046e6978ce91e10548d3b\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   hostname: ray-61bce103fa3de41ad2934902-ray-worker-2\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   mean_accuracy: 0.575\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   node_ip: 10.0.38.75\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   pid: 112\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   should_checkpoint: true\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_since_restore: 5.652081727981567\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_this_iter_s: 5.652081727981567\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_total_s: 5.652081727981567\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   timestamp: 1639768461\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   trial_id: 83e1d_00000\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result for WrappedDistributedTorchTrainable_83e1d_00000:\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   date: 2021-12-17_19-14-21\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   experiment_id: 843829f8718046e6978ce91e10548d3b\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   hostname: ray-61bce103fa3de41ad2934902-ray-worker-2\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   iterations_since_restore: 3\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   mean_accuracy: 0.8125\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   node_ip: 10.0.38.75\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   pid: 112\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_since_restore: 6.141621828079224\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_this_iter_s: 0.2401423454284668\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   time_total_s: 6.141621828079224\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   timestamp: 1639768461\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   training_iteration: 3\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   trial_id: 83e1d_00000\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current time: 2021-12-17 19:14:21 (running for 00:00:10.46)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Memory usage on this node: 3.3/60.0 GiB\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Resources requested: 0/35 CPUs, 0/5 GPUs, 0.0/186.81 GiB heap, 0.0/82.42 GiB objects (0.0/5.0 accelerator_type:V100)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current best trial: 83e1d_00000 with mean_accuracy=0.8125 and parameters={}\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result logdir: /home/ubuntu/ray_results/WrappedDistributedTorchTrainable_2021-12-17_19-14-11\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+------------+----------------+--------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | Trial name                                   | status     | loc            |    acc |   iter |   total time (s) |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m |----------------------------------------------+------------+----------------+--------+--------+------------------|\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | WrappedDistributedTorchTrainable_83e1d_00000 | TERMINATED | 10.0.38.75:112 | 0.8125 |      3 |          6.14162 |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+------------+----------------+--------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current time: 2021-12-17 19:14:21 (running for 00:00:10.47)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Memory usage on this node: 3.3/60.0 GiB\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Resources requested: 0/35 CPUs, 0/5 GPUs, 0.0/186.81 GiB heap, 0.0/82.42 GiB objects (0.0/5.0 accelerator_type:V100)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Current best trial: 83e1d_00000 with mean_accuracy=0.8125 and parameters={}\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Result logdir: /home/ubuntu/ray_results/WrappedDistributedTorchTrainable_2021-12-17_19-14-11\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+------------+----------------+--------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | Trial name                                   | status     | loc            |    acc |   iter |   total time (s) |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m |----------------------------------------------+------------+----------------+--------+--------+------------------|\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m | WrappedDistributedTorchTrainable_83e1d_00000 | TERMINATED | 10.0.38.75:112 | 0.8125 |      3 |          6.14162 |\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m +----------------------------------------------+------------+----------------+--------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m \n",
      "Best hyperparameters found were:  {}\n",
      "Best config:  {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=222)\u001b[0m 2021-12-17 19:14:21,606\tINFO tune.py:626 -- Total run time: 10.58 seconds (10.46 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ray.util.sgd import TorchTrainer\n",
    "from ray.util.sgd.torch import TrainingOperator\n",
    "from ray.tune import grid_search\n",
    "\n",
    "run_ddp_tune(\n",
    "        num_workers=2,\n",
    "        num_gpus_per_worker=1,\n",
    "        workers_per_node=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92cd4b6",
   "metadata": {},
   "source": [
    "#### What's next?\n",
    "\n",
    "*Try our Reinforcement Learning use case or our Beginner's Tutorial*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
